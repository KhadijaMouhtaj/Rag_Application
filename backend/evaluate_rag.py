"""
Script d'√©valuation RAG avec RAGAS
Mesure la qualit√© des r√©ponses g√©n√©r√©es par le syst√®me RAG
"""

from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_correctness
from openai import OpenAI
from ragas.llms import llm_factory
import requests
import os
import json
from datetime import datetime

from dotenv import load_dotenv

load_dotenv()


# ============================================================================
# CONFIGURATION
# ============================================================================

API_URL = "http://localhost:5000/ask"
RESULTS_DIR = "evaluation_results"

# Cl√© API OpenAI (pour l'√©valuateur RAGAS)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
# ============================================================================
# QUESTIONS DE TEST
# ============================================================================

TEST_QUESTIONS = [
    {
        "question": "Qu'est-ce que la reconnaissance faciale ?",
        "ground_truth": "La reconnaissance faciale est une technologie biom√©trique qui permet d'identifier ou de v√©rifier l'identit√© d'une personne √† partir des caract√©ristiques de son visage.",
        "category": "d√©finition"
    },
    {
        "question": "Comment fonctionne un algorithme CNN dans la reconnaissance faciale ?",
        "ground_truth": "Un CNN fonctionne en appliquant des couches de convolution pour extraire des caract√©ristiques hi√©rarchiques du visage, suivies de couches de pooling pour r√©duire la dimensionnalit√©, et enfin des couches fully connected pour la classification.",
        "category": "technique"
    },
    {
        "question": "Quelles sont les √©tapes principales d'un syst√®me biom√©trique de reconnaissance faciale ?",
        "ground_truth": "Les √©tapes principales sont : acquisition de l'image, pr√©traitement et normalisation, d√©tection et alignement du visage, extraction des caract√©ristiques, et comparaison/matching avec la base de donn√©es.",
        "category": "processus"
    },
]

# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def create_results_directory():
    """Cr√©e le dossier pour stocker les r√©sultats d'√©valuation"""
    if not os.path.exists(RESULTS_DIR):
        os.makedirs(RESULTS_DIR)
        print(f"üìÅ Dossier cr√©√©: {RESULTS_DIR}/")

def save_results(results_dict, filename=None):
    """Sauvegarde les r√©sultats dans un fichier JSON"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_{timestamp}.json"
    
    filepath = os.path.join(RESULTS_DIR, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, indent=2, ensure_ascii=False)
    
    print(f"üíæ R√©sultats sauvegard√©s: {filepath}")

def get_sources():
    """R√©cup√®re la liste des sources PDF upload√©es"""
    print("üìö R√©cup√©ration des sources disponibles...")
    try:
        sources_list = requests.get("http://localhost:5000/list_sources").json()
        selected_sources = [s["id"] for s in sources_list]
        
        print(f"‚úì {len(selected_sources)} source(s) trouv√©e(s)")
        for s in sources_list:
            print(f"  - {s['name']}: {s['chunks']} chunks")
        
        if not selected_sources:
            print("‚ö†Ô∏è  ATTENTION: Aucune source disponible!")
            print("   Uploadez des PDFs via l'interface avant d'√©valuer.")
            return None
        
        return selected_sources
    
    except Exception as e:
        print(f"‚ùå Erreur lors de la r√©cup√©ration des sources: {e}")
        print("   V√©rifiez que le serveur Flask est d√©marr√© (python app.py)")
        return None

def ask_rag(question, selected_sources):
    """Interroge le syst√®me RAG et r√©cup√®re la r√©ponse + contexte"""
    payload = {"question": question, "selected_ids": selected_sources}
    
    try:
        response = requests.post(API_URL, json=payload, timeout=60)
        
        if response.status_code != 200:
            print(f"  ‚ùå Erreur HTTP {response.status_code}")
            return None, []
        
        data = response.json()
        answer = data.get("answer", "")
        chunks = data.get("chunks", [])
        
        if not chunks:
            chunks = ["Aucun contexte trouv√©"]
        
        return answer, chunks
    
    except requests.exceptions.Timeout:
        print(f"  ‚è±Ô∏è  Timeout - La requ√™te a pris trop de temps")
        return None, []
    except Exception as e:
        print(f"  ‚ùå Erreur: {e}")
        return None, []

# ============================================================================
# √âVALUATION PRINCIPALE
# ============================================================================

def evaluate_rag(questions=None, save=True):
    """
    √âvalue le syst√®me RAG avec RAGAS
    
    Args:
        questions: Liste de questions √† tester (utilise TEST_QUESTIONS par d√©faut)
        save: Si True, sauvegarde les r√©sultats dans un fichier
    """
    
    if questions is None:
        questions = TEST_QUESTIONS
    
    # Cr√©er le dossier de r√©sultats
    if save:
        create_results_directory()
    
    # R√©cup√©rer les sources
    selected_sources = get_sources()
    if not selected_sources:
        return None
    
    # Pr√©parer les listes pour le dataset
    test_questions = []
    ground_truths = []
    answers = []
    contexts = []
    categories = []
    
    # Interroger le RAG pour chaque question
    print(f"\nüîÑ Interrogation du RAG ({len(questions)} questions)...")
    print("=" * 70)
    
    for i, item in enumerate(questions, 1):
        question = item["question"]
        ground_truth = item["ground_truth"]
        category = item.get("category", "g√©n√©ral")
        
        print(f"\nüìù Question {i}/{len(questions)}")
        print(f"   Cat√©gorie: {category}")
        print(f"   Q: {question[:70]}...")
        
        answer, chunks = ask_rag(question, selected_sources)
        
        if answer is None:
            print(f"   ‚ùå √âchec - Question ignor√©e")
            continue
        
        test_questions.append(question)
        ground_truths.append(ground_truth)
        answers.append(answer)
        contexts.append(chunks)
        categories.append(category)
        
        print(f"   ‚úì R√©ponse: {answer[:80]}...")
        print(f"   ‚úì Contexte: {len(chunks)} chunk(s)")
    
    if not test_questions:
        print("\n‚ùå Aucune question n'a pu √™tre trait√©e")
        return None
    
    # Cr√©er le dataset RAGAS
    print("\nüìä Pr√©paration du dataset RAGAS...")
    dataset = Dataset.from_dict({
        "question": test_questions,
        "answer": answers,
        "contexts": contexts,
        "ground_truth": ground_truths,
    })
    print(f"‚úì Dataset cr√©√©: {len(dataset)} exemples valid√©s")
    
    # Configuration du mod√®le d'√©valuation
    print("\nü§ñ Configuration du mod√®le d'√©valuation...")
    openai_client = OpenAI(api_key=OPENAI_API_KEY)
    evaluator_llm = llm_factory(
        "gpt-4o-mini",
        client=openai_client,
        temperature=0,
        max_tokens=4096
    )
    
    # Lancement de l'√©valuation
    print("\nüî¨ Lancement de l'√©valuation RAGAS...")
    print("M√©trique principale:")
    print("  - faithfulness: Fid√©lit√© au contexte (0-1)")
    print("    ‚Üí Mesure si le RAG g√©n√®re des hallucinations")
    print("    ‚Üí C'est LA m√©trique critique pour la production")
    print("\nüí° Note: answer_correctness d√©sactiv√©e (probl√®mes d'embeddings)")
    print("‚è≥ Patientez, cela peut prendre quelques minutes...\n")
    
    try:
        results = evaluate(
            dataset,
            metrics=[
                faithfulness,  # Seule m√©trique fiable
            ],
            llm=evaluator_llm,
        )
        
        # Affichage des r√©sultats
        print("\n" + "=" * 70)
        print("üìà R√âSULTATS DE L'√âVALUATION RAGAS")
        print("=" * 70)
        
        if hasattr(results, 'to_pandas'):
            df = results.to_pandas()
            
            # Scores moyens
            print("\nüìä SCORES MOYENS PAR M√âTRIQUE")
            print("-" * 70)
            
            results_dict = {
                "timestamp": datetime.now().isoformat(),
                "total_questions": len(dataset),
                "metrics": {},
                "questions": []
            }
            
            for metric in ['faithfulness']:
                if metric in df.columns:
                    score = df[metric].mean()
                    emoji = "üü¢" if score > 0.7 else "üü°" if score > 0.5 else "üî¥"
                    
                    print(f"\n{emoji} {metric.upper()}")
                    print(f"   Score: {score:.4f} ({score*100:.2f}%)")
                    
                    if score > 0.8:
                        interpretation = "Excellent ! Pas d'hallucinations"
                        print(f"   ‚Üí {interpretation}")
                        print(f"   ‚Üí Le RAG reste fid√®le aux documents sources")
                    elif score > 0.6:
                        interpretation = "Bon. Quelques d√©viations mineures"
                        print(f"   ‚Üí {interpretation}")
                        print(f"   ‚Üí Le mod√®le s'√©loigne parfois l√©g√®rement du contexte")
                    else:
                        interpretation = "√Ä am√©liorer. Le mod√®le invente des infos"
                        print(f"   ‚Üí {interpretation}")
                        print(f"   ‚Üí Risque d'hallucinations √©lev√©")
                    
                    results_dict["metrics"][metric] = {
                        "score": float(score),
                        "interpretation": interpretation
                    }
            
            # D√©tails par question
            print("\nüìã D√âTAILS PAR QUESTION")
            print("-" * 70)
            
            for idx, row in df.iterrows():
                print(f"\n‚ùì Question {idx + 1}: {test_questions[idx][:60]}...")
                print(f"   Cat√©gorie: {categories[idx]}")
                
                question_results = {
                    "question": test_questions[idx],
                    "category": categories[idx],
                    "answer": answers[idx][:200] + "...",
                    "scores": {}
                }
                
                if 'faithfulness' in row:
                    score = row['faithfulness']
                    emoji = "üü¢" if score > 0.7 else "üü°" if score > 0.5 else "üî¥"
                    print(f"   {emoji} Faithfulness: {score:.4f}")
                    question_results["scores"]["faithfulness"] = float(score)
                
                results_dict["questions"].append(question_results)
            
            # Analyse finale
            print("\n" + "=" * 70)
            print("üí° ANALYSE FINALE")
            print("=" * 70)
            
            faith_mean = df['faithfulness'].mean() if 'faithfulness' in df.columns else 0
            
            print(f"\nüéØ Fid√©lit√© au contexte (Faithfulness) : {faith_mean:.2%}")
            
            # Verdict bas√© uniquement sur faithfulness
            if faith_mean > 0.9:
                final_verdict = "EXCELLENT RAG - Pr√™t pour la production !"
                print(f"\nüåü {final_verdict}")
                print("   ‚úì Fid√©lit√© exceptionnelle au contexte")
                print("   ‚úì Pas d'hallucinations d√©tect√©es")
                print("   ‚úì Le syst√®me peut √™tre d√©ploy√© en confiance")
            elif faith_mean > 0.8:
                final_verdict = "TR√àS BON RAG - Quasi production-ready"
                print(f"\nüü¢ {final_verdict}")
                print("   ‚úì Tr√®s bonne fid√©lit√© au contexte")
                print("   ‚úì Hallucinations tr√®s rares")
                print("   ‚Üí Quelques tests suppl√©mentaires recommand√©s")
            elif faith_mean > 0.7:
                final_verdict = "BON RAG - Optimisations recommand√©es"
                print(f"\n‚úÖ {final_verdict}")
                print("   ‚úì Bonne fid√©lit√© g√©n√©rale au contexte")
                print("   ‚ö†Ô∏è  Quelques d√©viations occasionnelles")
                print("   ‚Üí Am√©liorer le prompt pour renforcer la fid√©lit√©")
            else:
                final_verdict = "RAG √Ä AM√âLIORER - Risque d'hallucinations"
                print(f"\n‚ö†Ô∏è  {final_verdict}")
                print("   ‚ùå Fid√©lit√© insuffisante au contexte")
                print("   ‚ùå Risque d'hallucinations trop √©lev√©")
                print("   ‚Üí Actions urgentes :")
                print("      1. Revoir le prompt syst√®me")
                print("      2. Am√©liorer la qualit√© du retrieval")
                print("      3. Augmenter k (nombre de chunks)")
                print("      4. Utiliser un mod√®le LLM plus puissant")
            
            results_dict["final_verdict"] = final_verdict
            results_dict["recommendations"] = []
            
            # Recommandations sp√©cifiques par question
            print("\nüìù RECOMMANDATIONS PAR QUESTION:")
            problematic_questions = [
                (idx, categories[idx], row['faithfulness']) 
                for idx, row in df.iterrows() 
                if 'faithfulness' in row and row['faithfulness'] < 0.8
            ]
            
            if problematic_questions:
                print("   Questions avec score < 0.8 :")
                for idx, cat, score in problematic_questions:
                    print(f"   ‚Ä¢ Q{idx+1} ({cat}): {score:.2%} - √Ä am√©liorer")
                    results_dict["recommendations"].append({
                        "question_id": idx + 1,
                        "category": cat,
                        "score": float(score),
                        "issue": "Fid√©lit√© insuffisante"
                    })
            else:
                print("   ‚úì Toutes les questions ont un score > 0.8")
                print("   ‚úì Pas de recommandations sp√©cifiques")
            
            # Score moyen par cat√©gorie
            print("\nüìä SCORES PAR CAT√âGORIE:")
            category_scores = {}
            for idx, row in df.iterrows():
                cat = categories[idx]
                if cat not in category_scores:
                    category_scores[cat] = []
                if 'faithfulness' in row:
                    category_scores[cat].append(row['faithfulness'])
            
            for cat, scores in sorted(category_scores.items()):
                avg = sum(scores) / len(scores)
                emoji = "üü¢" if avg > 0.8 else "üü°" if avg > 0.6 else "üî¥"
                print(f"   {emoji} {cat:<20}: {avg:.2%} ({len(scores)} question(s))")
            
            results_dict["category_scores"] = {
                cat: float(sum(scores) / len(scores)) 
                for cat, scores in category_scores.items()
            }
            
            # Sauvegarder les r√©sultats
            if save:
                save_results(results_dict)
            
            print("\n" + "=" * 70)
            print("‚úÖ √âVALUATION TERMIN√âE")
            print("=" * 70)
            
            return results_dict
        
    except Exception as e:
        print(f"\n‚ùå Erreur pendant l'√©valuation: {e}")
        import traceback
        traceback.print_exc()
        return None

# ============================================================================
# POINT D'ENTR√âE
# ============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("üî¨ √âVALUATION RAG AVEC RAGAS")
    print("=" * 70)
    
    results = evaluate_rag()
    
    if results:
        print(f"\nüìä R√©sultats disponibles dans le dossier: {RESULTS_DIR}/")